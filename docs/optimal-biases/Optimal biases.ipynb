{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb29b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 12:56:10.056325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe4f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699b20a",
   "metadata": {},
   "source": [
    "In this experiment I'm going to show how to set the optimal bias initialization for your neural network. In particular, I'll use a subset of the cifat10 dataset, with unbalanced classes. Then, I'll train two network with this dataset, one with an optimized bias initialization and another one with a standard bias initialization.\n",
    "\n",
    "The mathematical details can be found in my blog post (https://www.amolas.dev/blog/bias-initialization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2371bbc",
   "metadata": {},
   "source": [
    "# Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f23e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f6f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = x_train.shape[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf41b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1c056",
   "metadata": {},
   "source": [
    "# Take a sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0321741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each class define how many examples do you want to get in the sampled dataset\n",
    "samples = {0: 4000,\n",
    "           1: 3700,\n",
    "           2: 100,\n",
    "           3: 4000,\n",
    "           4: 400,\n",
    "           5: 100,\n",
    "           6: 50,\n",
    "           7: 700,\n",
    "           8: 700,\n",
    "           9: 100}\n",
    "total = sum(samples.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a0aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "acc = {i:0 for i in range(10)}\n",
    "for image, label in zip(x_train, y_train):\n",
    "    if acc[label] < samples[label]:\n",
    "        X_train.append(image)\n",
    "        Y_train.append(label)\n",
    "        acc[label] += 1\n",
    "X_train = np.array(X_train)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619aab8",
   "metadata": {},
   "source": [
    "# Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f937b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard CNN approach.\n",
    "\n",
    "def get_model(optimal_bias=True):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    if optimal_bias:\n",
    "        initial_bias = tf.constant_initializer([np.log(samples[i]/total) for i in range(10)])\n",
    "        output_layer = tf.keras.layers.Dense(units=10, \n",
    "                                             bias_initializer=initial_bias,\n",
    "                                             activation='softmax')\n",
    "    else:\n",
    "        initial_bias = tf.constant_initializer([0 for i in range(10)])\n",
    "        output_layer = tf.keras.layers.Dense(units=10, \n",
    "                                             bias_initializer=initial_bias, \n",
    "                                             activation='softmax')\n",
    "    model.add(output_layer)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaed350e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 12:56:16.456415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Get the models\n",
    "normal_model = get_model(optimal_bias=False)\n",
    "opt_bias_model = get_model(optimal_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd317c",
   "metadata": {},
   "source": [
    "## Run some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959a675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-optimized model\n",
      "    Expected pre-training loss 2.3025850929940463\n",
      "    Actual pre-training loss 2.271348714828491\n"
     ]
    }
   ],
   "source": [
    "# expected loss from using a non-informative initialization\n",
    "exp = -sum([n_i/total * np.log(1/10) for n_i in samples.values()])\n",
    "\n",
    "# actual loss\n",
    "pretrain_preds = normal_model.predict(X_train, verbose=False)\n",
    "act = np.mean(tf.keras.losses.categorical_crossentropy(Y_train, pretrain_preds))\n",
    "\n",
    "print(\"Non-optimized model\")\n",
    "print(f\"    Expected pre-training loss {exp}\")\n",
    "print(f\"    Actual pre-training loss {act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd83ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-optimized model\n",
      "    Expected pre-training loss 1.6012265347398464\n",
      "    Actual pre-training loss 1.5987855195999146\n"
     ]
    }
   ],
   "source": [
    "# expected loss from using an optimal initialization\n",
    "exp = -sum([n_i/total * np.log(n_i/total) for n_i in samples.values()])\n",
    "\n",
    "# actual loss\n",
    "pretrain_preds = opt_bias_model.predict(X_train, verbose=False)\n",
    "act = np.mean(tf.keras.losses.categorical_crossentropy(Y_train, pretrain_preds))\n",
    "\n",
    "print(\"Non-optimized model\")\n",
    "print(f\"    Expected pre-training loss {exp}\")\n",
    "print(f\"    Actual pre-training loss {act}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ff90f",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8534a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define this callback to have access to training loss at every batch\n",
    "\n",
    "class BatchLossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.batch_losses = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        self.batch_losses.append(logs.get('loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eff324",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 6s 89ms/step - loss: 1.5134 - accuracy: 0.4248\n",
      "13/55 [======>.......................] - ETA: 3s - loss: 1.6005 - accuracy: 0.3407"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "normal_callback = BatchLossCallback()\n",
    "normal_history = normal_model.fit(X_train, \n",
    "                                  Y_train, \n",
    "                                  batch_size=batch_size, \n",
    "                                  epochs=epochs, \n",
    "                                  callbacks=[normal_callback])\n",
    "\n",
    "opt_bias_callback = BatchLossCallback()\n",
    "opt_bias_history = opt_bias_model.fit(X_train, \n",
    "                                      Y_train, \n",
    "                                      batch_size=batch_size, \n",
    "                                      epochs=epochs, \n",
    "                                      callbacks=[opt_bias_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f64c0e",
   "metadata": {},
   "source": [
    "# Show the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dea071",
   "metadata": {},
   "source": [
    "In the plot below we see that the optimized model converges faster and the initial loss is lower that the non-optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08028f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(opt_bias_callback.batch_losses, 'k--', label=\"optimal initial bias\")\n",
    "plt.plot(normal_callback.batch_losses, 'k-', label=\"non-optimal initial bias\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"opt-vs-normal-loss.svg\", bbox_inches = \"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f20b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
